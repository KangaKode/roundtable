"""Unit tests for orchestration -- round table, chat orchestrator, agent router."""

import pytest
from unittest.mock import AsyncMock

from src.{{project_slug}}.orchestration.round_table import RoundTable, RoundTableConfig, RoundTableTask
from src.{{project_slug}}.orchestration.chat_orchestrator import ChatOrchestrator, ChatConfig
from src.{{project_slug}}.orchestration.agent_router import AgentRouter


class TestAgentRouter:
    def test_route_selects_by_domain(self, mock_registry):
        router = AgentRouter(registry=mock_registry)
        decision = router.route("analyze the code for bugs")
        assert len(decision.selected_agents) > 0

    def test_route_respects_trust_scores(self, mock_registry):
        router = AgentRouter(registry=mock_registry)
        trust = {"analyst_a": 0.9, "analyst_b": 0.1}
        decision = router.route("test query", trust_scores=trust)
        if len(decision.selected_agents) > 0:
            assert decision.selected_agents[0].name == "analyst_a"

    def test_route_escalates_with_no_agents(self):
        from pathlib import Path
        from src.{{project_slug}}.agents.registry import AgentRegistry
        empty_registry = AgentRegistry(persist_path=Path("/tmp/empty_agents.json"))
        router = AgentRouter(registry=empty_registry)
        decision = router.route("test query")
        assert decision.should_escalate is True

    def test_route_respects_max_agents(self, mock_registry):
        router = AgentRouter(registry=mock_registry, max_agents=1)
        decision = router.route("test query")
        assert len(decision.selected_agents) <= 1

    def test_route_with_llm_hint_validates_names(self, mock_registry):
        router = AgentRouter(registry=mock_registry)
        decision = router.route_with_llm_hint(
            "test query", llm_suggested_agents=["analyst_a", "analyst_b"]
        )
        names = [a.name for a in decision.selected_agents]
        assert "analyst_a" in names
        assert "analyst_b" in names

    def test_route_with_llm_hint_ignores_invalid(self, mock_registry):
        router = AgentRouter(registry=mock_registry)
        decision = router.route_with_llm_hint(
            "test query", llm_suggested_agents=["nonexistent", "analyst_a"]
        )
        names = [a.name for a in decision.selected_agents]
        assert "analyst_a" in names
        assert "nonexistent" not in names

    def test_route_with_llm_hint_falls_back_when_insufficient(self, mock_registry):
        router = AgentRouter(registry=mock_registry, min_agents=2)
        decision = router.route_with_llm_hint(
            "analyze code quality", llm_suggested_agents=["analyst_a"]
        )
        assert len(decision.selected_agents) >= 2

    def test_route_with_llm_hint_respects_max(self, mock_registry):
        router = AgentRouter(registry=mock_registry, max_agents=1)
        decision = router.route_with_llm_hint(
            "test", llm_suggested_agents=["analyst_a", "analyst_b"]
        )
        assert len(decision.selected_agents) <= 1

    def test_route_with_llm_hint_none_registry(self):
        router = AgentRouter(registry=None)
        decision = router.route_with_llm_hint(
            "test", llm_suggested_agents=["analyst_a"]
        )
        assert decision.should_escalate is True


class TestRoundTable:
    @pytest.mark.asyncio
    async def test_run_produces_result(self, mock_agents, mock_llm, sample_task):
        config = RoundTableConfig(
            enable_strategy_phase=False,
            enable_challenge_phase=False,
        )
        rt = RoundTable(agents=mock_agents, config=config, llm_client=mock_llm)
        result = await rt.run(sample_task)
        assert result.task_id == sample_task.id
        assert len(result.analyses) == 7  # 5 core + 2 user agents

    @pytest.mark.asyncio
    async def test_consensus_threshold(self, mock_agents, mock_llm, sample_task):
        config = RoundTableConfig(
            enable_strategy_phase=False,
            enable_challenge_phase=False,
            consensus_threshold=0.5,
            include_core_agents=False,
        )
        rt = RoundTable(agents=mock_agents, config=config, llm_client=mock_llm)
        result = await rt.run(sample_task)
        assert result.consensus_reached is True
        assert result.approval_rate >= 0.5

    @pytest.mark.asyncio
    async def test_strategy_wires_into_context(self, mock_agents, mock_llm, sample_task):
        config = RoundTableConfig(
            enable_strategy_phase=True,
            enable_challenge_phase=False,
        )
        mock_llm.call.return_value.content = '{"task_decomposition": ["step1"], "agent_focus_areas": {"analyst_a": "focus_a"}, "anticipated_tensions": [], "success_criteria": []}'
        rt = RoundTable(agents=mock_agents, config=config, llm_client=mock_llm)
        result = await rt.run(sample_task)
        assert result.task_id == sample_task.id


class TestChatOrchestrator:
    @pytest.mark.asyncio
    async def test_chat_produces_response(self, mock_llm, mock_registry):
        orchestrator = ChatOrchestrator(
            llm=mock_llm,
            registry=mock_registry,
        )
        response = await orchestrator.chat("What is testing?")
        assert response.content is not None
        assert len(response.content) > 0

    @pytest.mark.asyncio
    async def test_chat_tracks_history(self, mock_llm, mock_registry):
        orchestrator = ChatOrchestrator(llm=mock_llm, registry=mock_registry)
        await orchestrator.chat("First message")
        await orchestrator.chat("Second message")
        assert orchestrator.history_length == 4  # 2 user + 2 assistant

    @pytest.mark.asyncio
    async def test_clear_history(self, mock_llm, mock_registry):
        orchestrator = ChatOrchestrator(llm=mock_llm, registry=mock_registry)
        await orchestrator.chat("message")
        orchestrator.clear_history()
        assert orchestrator.history_length == 0


class TestSessionKey:
    def test_session_key_binds_to_tenant_and_user(self):
        from src.{{project_slug}}.api.routes.chat import _session_key
        from src.{{project_slug}}.api.middleware.auth import AuthContext
        auth = AuthContext(user_id="user_abc", tenant_id="acme")
        key = _session_key("my_session", auth)
        assert key == "acme:user_abc:my_session"

    def test_session_key_default_tenant(self):
        from src.{{project_slug}}.api.routes.chat import _session_key
        from src.{{project_slug}}.api.middleware.auth import AuthContext
        auth = AuthContext()
        key = _session_key("my_session", auth)
        assert key == "default:anon:my_session"


# =============================================================================
# ROUND TABLE SYNTHESIS ERROR HANDLING
# =============================================================================


class TestCoreAgents:
    @pytest.mark.asyncio
    async def test_core_agents_auto_included(self, mock_agents, mock_llm):
        """Core agents are automatically added when include_core_agents=True."""
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
            include_core_agents=True,
        ), llm_client=mock_llm)
        agent_names = [a.name for a in rt.agents]
        assert "skeptic" in agent_names
        assert "quality" in agent_names
        assert "evidence" in agent_names
        assert "fact_checker" in agent_names
        assert "citation" in agent_names
        assert len(rt.agents) == len(mock_agents) + 5

    @pytest.mark.asyncio
    async def test_core_agents_opt_out(self, mock_agents, mock_llm):
        """Core agents excluded when include_core_agents=False."""
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
            include_core_agents=False,
        ), llm_client=mock_llm)
        agent_names = [a.name for a in rt.agents]
        assert "skeptic" not in agent_names
        assert "quality" not in agent_names
        assert "evidence" not in agent_names
        assert len(rt.agents) == len(mock_agents)

    @pytest.mark.asyncio
    async def test_core_agents_participate_in_round_table(self, mock_agents, mock_llm):
        """Core agents produce analyses alongside user agents."""
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
            include_core_agents=True,
        ), llm_client=mock_llm)
        task = RoundTableTask(id="core_test", content="Test with core agents")
        result = await rt.run(task)
        analysis_names = [a.agent_name for a in result.analyses]
        assert "skeptic" in analysis_names
        assert "quality" in analysis_names
        assert "evidence" in analysis_names

    @pytest.mark.asyncio
    async def test_core_agents_without_llm(self, mock_agents):
        """Core agents produce placeholder analyses without LLM."""
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
            include_core_agents=True,
        ))
        task = RoundTableTask(id="no_llm_test", content="Test without LLM")
        result = await rt.run(task)
        core_analyses = [a for a in result.analyses if a.agent_name in ("skeptic", "quality", "evidence", "fact_checker", "citation")]
        assert len(core_analyses) == 5
        for a in core_analyses:
            assert len(a.observations) > 0


class TestRoundTableSynthesis:
    @pytest.mark.asyncio
    async def test_synthesis_no_llm(self, mock_agents):
        """Synthesis gracefully handles no LLM client."""
        from src.{{project_slug}}.orchestration.round_table import SynthesisResult

        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
        ))
        task = RoundTableTask(id="test_no_llm", content="test")
        result = await rt.run(task)
        assert result.synthesis is not None
        assert "No LLM" in result.synthesis.recommended_direction

    @pytest.mark.asyncio
    async def test_synthesis_malformed_json(self, mock_agents, mock_llm):
        """Synthesis handles LLM returning non-JSON content."""
        from src.{{project_slug}}.llm.client import LLMResponse, TokenUsage

        mock_llm.call = AsyncMock(return_value=LLMResponse(
            content="This is not JSON at all, just plain text analysis.",
            usage=TokenUsage(input_tokens=10, output_tokens=20),
            model="test", provider="test",
        ))
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
        ), llm_client=mock_llm)
        task = RoundTableTask(id="test_bad_json", content="test")
        result = await rt.run(task)
        assert result.synthesis is not None
        assert len(result.synthesis.recommended_direction) > 0

    @pytest.mark.asyncio
    async def test_synthesis_wrong_keys(self, mock_agents, mock_llm):
        """Synthesis handles JSON with unexpected structure."""
        import json
        from src.{{project_slug}}.llm.client import LLMResponse, TokenUsage

        mock_llm.call = AsyncMock(return_value=LLMResponse(
            content=json.dumps({"unexpected_key": "value", "another": [1, 2]}),
            usage=TokenUsage(input_tokens=10, output_tokens=20),
            model="test", provider="test",
        ))
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
        ), llm_client=mock_llm)
        task = RoundTableTask(id="test_wrong_keys", content="test")
        result = await rt.run(task)
        assert result.synthesis is not None

    @pytest.mark.asyncio
    async def test_synthesis_llm_failure(self, mock_agents, mock_llm):
        """Synthesis handles LLM call raising an exception."""
        mock_llm.call = AsyncMock(side_effect=RuntimeError("LLM crashed"))
        rt = RoundTable(agents=mock_agents, config=RoundTableConfig(
            enable_strategy_phase=False, enable_challenge_phase=False,
        ), llm_client=mock_llm)
        task = RoundTableTask(id="test_llm_fail", content="test")
        result = await rt.run(task)
        assert result.synthesis is not None
        assert "failed" in result.synthesis.recommended_direction.lower()
